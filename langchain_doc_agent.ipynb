{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDrLbQtREBa3"
   },
   "source": [
    "# Ask Docs Agent\n",
    "\n",
    "**Note**: This notebook is not free to run, you will need to create ~20K OpenAI `text-embedding-ada-002` embedding which do cost money. The Pinecone index can be run within the free tier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pADty8WtkcGq"
   },
   "outputs": [],
   "source": [
    "!pip install -qU datasets pod-gpt pinecone-client[grpc] langchain openai tqdm PyPDF2 tiktoken matplotlib seaborn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xQLZnwXaLOc1",
    "outputId": "9c303412-4942-4052-b9f9-c16f74b57bab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n5hiL-7BK6O3",
    "outputId": "33405544-31bc-4cbe-d075-8d664e107211"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.file-revisions-by-id', 'MyDrive', '.shortcut-targets-by-id', '.Trash-0']\n"
     ]
    }
   ],
   "source": [
    "directory_path = '/content/drive'\n",
    "directory_files = os.listdir(directory_path)\n",
    "\n",
    "print(directory_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYzuZiAP5HGL"
   },
   "source": [
    "Set api keys, etc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1R8X32-41vc"
   },
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"\"  # platform.openai.com\n",
    "PINECONE_API_KEY = \"\"  # app.pinecone.io\n",
    "PINECONE_ENV = \"us-east4-gcp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jYNN8n8kJYiU"
   },
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "books = ['Postnihilism v12.pdf',\n",
    "         'Natural Language Cognitive Architecture v7.pdf',\n",
    "         'Benevolent By Design v8.pdf',\n",
    "         'Symphony of Thought v3.pdf']\n",
    "\n",
    "texts = []\n",
    "\n",
    "for book in books:\n",
    "\n",
    "  # open PDF file\n",
    "  with open('/content/drive/MyDrive/Chatbot/' + book, 'rb') as file:\n",
    "      # create a PDF reader object\n",
    "      reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "      # get the number of pages in the PDF file\n",
    "      num_pages = len(reader.pages)\n",
    "\n",
    "      book_text = ''\n",
    "\n",
    "      # loop through each page and extract the text\n",
    "      for i in range(num_pages):\n",
    "          # get the current page object\n",
    "          page = reader.pages[i]\n",
    "\n",
    "          # extract the text from the page\n",
    "          book_text += page.extract_text()\n",
    "\n",
    "      texts.append(book_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vvct9LuORbj2"
   },
   "source": [
    "Set up tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u95OLQudRfaB"
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "# create the length function\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        disallowed_special=()\n",
    "    )\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_oS3A0xkRkMC",
    "outputId": "9b01197d-c67e-4e1b-fa41-bc5999667274"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'cl100k_base'>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiktoken.encoding_for_model('gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GmpU6B_FRmjW"
   },
   "outputs": [],
   "source": [
    "token_counts = [tiktoken_len(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g7ohFYnRRxWE",
    "outputId": "670e6d5a-aed0-4e03-abc1-93bfa187f2f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 23843\n",
      "Avg: 59115\n",
      "Max: 73633\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Min: {min(token_counts)}\n",
    "Avg: {int(sum(token_counts) / len(token_counts))}\n",
    "Max: {max(token_counts)}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qodmYsUKS5WL"
   },
   "source": [
    "Calculate the total price to make embeddings for this text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Kv2y1_lSkZ7",
    "outputId": "dedd6820-4e47-4996-9c8b-afa61f2d2afe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ 0.0944\n"
     ]
    }
   ],
   "source": [
    "price = 0.0004 # price per 1000 tokens for the ada model\n",
    "print('$ ' + str((price*int(sum(token_counts)/1000))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foKznOYWTful"
   },
   "source": [
    "Very Afordable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXR0Oli1TqxY"
   },
   "source": [
    "Now lets use a chunker to break the text into usable sizes for the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YWBtJn7dTyTu"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=20,  # number of tokens overlap between chunks\n",
    "    length_function=tiktoken_len,\n",
    "    separators=['\\n\\n', '\\n', ' ', '']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6TYgtXA9T0PX",
    "outputId": "64c22a3c-9deb-407d-9038-e623bbae9187"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = text_splitter.split_text(texts[3])\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "240jEXAVWPx1"
   },
   "source": [
    "Process the data into chucks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "71b1c2ce3c014048b3eb351ef9d2b0b7",
      "7b9331b9c35142f396c8df6b2fc69142",
      "5e13d888ee6b40f18f8fcf7530a6f703",
      "20483e8d037b49c295c185149e2ba2ce",
      "ce0640b089774bebbb485895abb5b6a0",
      "98fd3d2619704051812999c9b00afae5",
      "28c4a0baf51f4cdb8718ada1c518b929",
      "85fbc34b2cf643739490adaa6098add1",
      "8ff451546dab4413a22926c1d0ef626b",
      "07b62ec44bb64d94b04859a54307167b",
      "983a80897b844c85b4bc118d81c6617f"
     ]
    },
    "id": "SH09YwmkWMqW",
    "outputId": "7dfce8c5-bf7b-453f-9941-b337efd353a0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b1c2ce3c014048b3eb351ef9d2b0b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for idx, record in enumerate(tqdm(texts)):\n",
    "    split_text = text_splitter.split_text(record)\n",
    "    chunks.extend([{\n",
    "        'id': str(uuid4()),\n",
    "        'text': split_text[i],\n",
    "        'chunk': i\n",
    "    } for i in range(len(split_text))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nmYoqbUUXA6Z",
    "outputId": "44e03f54-a567-4150-ccec-e406739beb90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "665"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSqAdgAxaTBq"
   },
   "source": [
    "Initialize the embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n8G8HHfvaWBi"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# initialize openai API key\n",
    "openai.api_key = OPENAI_API_KEY  #platform.openai.com\n",
    "\n",
    "embed_model = \"text-embedding-ada-002\"\n",
    "\n",
    "res = openai.Embedding.create(\n",
    "    input=[\n",
    "        \"Sample document text goes here\",\n",
    "        \"there will be several phrases in each batch\"\n",
    "    ], engine=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4awdgi1vaysj"
   },
   "source": [
    "In the response res we will find a JSON-like object containing our new embeddings within the 'data' field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kK2zQoz-af7d",
    "outputId": "cc82acc2-41c8-4d11-b558-862873af6c93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['object', 'data', 'model', 'usage'])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.keys()\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxWdPtY0a3CX"
   },
   "source": [
    "Inside 'data' we will find two records, one for each of the two sentences we just embedded. Each vector embedding contains 1536 dimensions (the output dimensionality of the text-embedding-ada-002 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wo_tgdtfa5m-",
    "outputId": "2dffa2cb-9777-4c91-bd15-bbf82024057c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rDGlHHuja93l",
    "outputId": "8e4744a8-e800-445a-ef53-e9a956bb280d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1536, 1536)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res['data'][0]['embedding']), len(res['data'][1]['embedding'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYKJLQwPbMmw"
   },
   "source": [
    "We will apply this same embedding logic to the books that have been chunked. But before doing so we must create a place to store the embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDCV-1iebTyz"
   },
   "source": [
    "# Initializing the Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMn2UTg1bbBt"
   },
   "source": [
    "Now we need a place to store these embeddings and enable a efficient vector search through them all. To do that we use Pinecone, we can get a free API key and enter it below where we will initialize our connection to Pinecone and create a new index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yi3TGwD8bWf5",
    "outputId": "1f2dde01-3083-40ed-cfd1-59d93eac03ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pinecone\n",
    "\n",
    "index_name = 'ask-ben'\n",
    "\n",
    "# initialize connection to pinecone\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,  # app.pinecone.io (console)\n",
    "    environment=PINECONE_ENV  # next to API key in console\n",
    ")\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    # if does not exist, create index\n",
    "    pinecone.create_index(\n",
    "        index_name,\n",
    "        dimension=len(res['data'][0]['embedding']),\n",
    "        metric='dotproduct'\n",
    "    )\n",
    "# connect to index\n",
    "index = pinecone.GRPCIndex(index_name)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBhVr3PrbyZp"
   },
   "source": [
    "We can see the index is currently empty with a total_vector_count of 0. We can begin populating it with OpenAI text-embedding-ada-002 built embeddings like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "e0678b27edb64f62aaedbfee011be1ee",
      "4e207721fd914c30bd176393a3b43360",
      "4d017abbab6a4607b8c98c1874b14fdb",
      "7759081e382942ba91fa419b246e9f38",
      "dee1b73d01e449ef8088048b1dc5cbd3",
      "b7f761f3cf5542ad89eac15a40dfab43",
      "d7a76b188cf94565bc760b275c24c72b",
      "ae05a5fda3364eeab7a9a5a233df299d",
      "b60d5e513c9649ccab44969745151342",
      "908ceb3775984baa9f462a6db7611c7e",
      "2b4b7dbb07a34daa8951422fe9237cfc"
     ]
    },
    "id": "Yky24tA3b18L",
    "outputId": "93a58c82-9429-4325-e429-34ebc4272063"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0678b27edb64f62aaedbfee011be1ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import datetime\n",
    "from time import sleep\n",
    "\n",
    "batch_size = 100  # how many embeddings we create and insert at once\n",
    "\n",
    "for i in tqdm(range(0, len(chunks), batch_size)):\n",
    "    # find end of batch\n",
    "    i_end = min(len(chunks), i+batch_size)\n",
    "    meta_batch = chunks[i:i_end]\n",
    "    # get ids\n",
    "    ids_batch = [x['id'] for x in meta_batch]\n",
    "    # get texts to encode\n",
    "    texts = [x['text'] for x in meta_batch]\n",
    "    # create embeddings (try-except added to avoid RateLimitError)\n",
    "    try:\n",
    "        res = openai.Embedding.create(input=texts, engine=embed_model)\n",
    "    except:\n",
    "        done = False\n",
    "        while not done:\n",
    "            sleep(5)\n",
    "            try:\n",
    "                res = openai.Embedding.create(input=texts, engine=embed_model)\n",
    "                done = True\n",
    "            except:\n",
    "                pass\n",
    "    embeds = [record['embedding'] for record in res['data']]\n",
    "    # cleanup metadata\n",
    "    meta_batch = [{\n",
    "        'text': x['text'],\n",
    "        'chunk': x['chunk']\n",
    "    } for x in meta_batch]\n",
    "    to_upsert = list(zip(ids_batch, embeds, meta_batch))\n",
    "    # upsert to Pinecone\n",
    "    index.upsert(vectors=to_upsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vXhlFwK2cMYw",
    "outputId": "20f6f031-b04a-4803-90dd-4832b23abfb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 665}},\n",
       " 'total_vector_count': 665}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_tGDMBub9-K"
   },
   "source": [
    "Now we've added all of our books to the index. With that we can move on to retrieval and then answer generation using GPT-3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCTQAo-scWDs"
   },
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgpAAfoBcYDx"
   },
   "source": [
    "To search through our documents we first need to create a query vector xq. Using xq we will retrieve the most relevant chunks from the books, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PV95e5opccT0"
   },
   "outputs": [],
   "source": [
    "query = \"What is the most likely outcome with the invention of AGI\"\n",
    "\n",
    "res = openai.Embedding.create(\n",
    "    input=[query],\n",
    "    engine=embed_model\n",
    ")\n",
    "\n",
    "# retrieve from Pinecone\n",
    "xq = res['data'][0]['embedding']\n",
    "\n",
    "# get relevant contexts (including the questions)\n",
    "res = index.query(xq, top_k=5, include_metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i6-MpkvocmsB",
    "outputId": "f45c53d7-747a-45aa-e225-87ca9308d688"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': 'c3362826-44a1-4d8a-98a2-52181c66f103',\n",
       "              'metadata': {'chunk': 153.0,\n",
       "                           'text': 'Many people I ’ve talked to do not want a '\n",
       "                                   'super powerful AGI to exist ever. \\n'\n",
       "                                   'Under no circumstances do they want Skynet '\n",
       "                                   'to exist, even if it is benevolent by \\n'\n",
       "                                   'design. While I understand their '\n",
       "                                   'sentiment, I think we must  accept the \\n'\n",
       "                                   'possibility that an arms race between nati '\n",
       "                                   'ons will necessitate the creation of ever '\n",
       "                                   '-\\n'\n",
       "                                   'more -powerful AGI systems. Similarly, we '\n",
       "                                   'would rather live in a world without \\n'\n",
       "                                   'nuclear weapons or other WMD, but they '\n",
       "                                   'exist and so we must learn to live \\n'\n",
       "                                   'with it. In the case of AGI, it can exist '\n",
       "                                   'and therefore it pro bably will exist, so '\n",
       "                                   'we \\n'\n",
       "                                   'must  adapt and do it right . Instead of '\n",
       "                                   'just tolerating a new technology, it '\n",
       "                                   'would \\n'\n",
       "                                   'be best to create a supremely beneficial '\n",
       "                                   'version of that technology , we don’t \\n'\n",
       "                                   'want to “just live with AGI,” we want it '\n",
       "                                   'to make everyone’s life way bet ter. \\n'\n",
       "                                   'Humans often rail against new technology, '\n",
       "                                   'especially when they don’t \\n'\n",
       "                                   'understand it. My Core Objective Functions '\n",
       "                                   'are no different: most people \\n'\n",
       "                                   'struggle to understand what they are and '\n",
       "                                   'how they work (another reason I’m \\n'\n",
       "                                   'writing this book!) Suspicion of n ew '\n",
       "                                   'technologies is healthy and necessary, and '\n",
       "                                   'I \\n'\n",
       "                                   'welcome it. Look at plastics, which have '\n",
       "                                   'been poisoning us with parabens and \\n'\n",
       "                                   'phthalates for many years. Even the most '\n",
       "                                   'innocuous inventions can have 152 | P a g '\n",
       "                                   'e    Benevolent by Design  \\n'\n",
       "                                   'profound repercussions unless we fully '\n",
       "                                   'understand them, and they earn our \\n'\n",
       "                                   'trust over time. Other technologies, like '\n",
       "                                   'Soviet -style communism, have rightly \\n'\n",
       "                                   'earned our mistrust  because that '\n",
       "                                   'invention turned out to be incredibly '\n",
       "                                   'harmful.  \\n'\n",
       "                                   'Trust is the first flaw of my Core '\n",
       "                                   'Objective Functions. Hold the AGI'},\n",
       "              'score': 0.8569356,\n",
       "              'sparse_values': {'indices': [], 'values': []},\n",
       "              'values': []},\n",
       "             {'id': 'b7909a2a-1899-4e64-97b9-b95ce0621c68',\n",
       "              'metadata': {'chunk': 146.0,\n",
       "                           'text': 'couple years ago, and over time, you save '\n",
       "                                   'up enough money to get your first \\n'\n",
       "                                   'mini-yacht.  \\n'\n",
       "                                   'All the while, the AGIs have gotten faster '\n",
       "                                   'and smarter, and all health issues \\n'\n",
       "                                   'are history. Cancer? Gone. Coronavirus? '\n",
       "                                   'Man, who even remembers that? Heart \\n'\n",
       "                                   'disease? We got a vaccine for that. '\n",
       "                                   'Dementia? Well, no one can help your in-\\n'\n",
       "                                   'laws with that, but that’s beside the '\n",
       "                                   'point.  \\n'\n",
       "                                   'We have now arrived at the singularity, '\n",
       "                                   'the point in time where all these \\n'\n",
       "                                   'compounding returns transform human '\n",
       "                                   'existence into something completely \\n'\n",
       "                                   'novel, that we can’t even yet imagine. A '\n",
       "                                   'benevolent AGI must be part of this  \\n'\n",
       "                                   'outcome . \\n'\n",
       "                                   'To take this one step further, let’s '\n",
       "                                   'imagine that every nation on th e planet \\n'\n",
       "                                   'implements AGI government. The AGI is '\n",
       "                                   'smarter, fairer, and kinder than any \\n'\n",
       "                                   'human leaders. Certainly, we still want '\n",
       "                                   'humans in the loop, but most \\n'\n",
       "                                   'government functions can be replaced by '\n",
       "                                   'AGI. Over many decades and \\n'\n",
       "                                   'centuries, the AGI helps humanity to  '\n",
       "                                   'understand each other and achieve an \\n'\n",
       "                                   'incomprehensible level of peace and '\n",
       "                                   'prosperity. Future generations grow up \\n'\n",
       "                                   'with AGI, like how children are growing up '\n",
       "                                   'with Alexa today. Global unity is \\n'\n",
       "                                   'achieved with the help of the Core '\n",
       "                                   'Objective Functions.  \\n'\n",
       "                                   'This utopi an vision is my goal with the '\n",
       "                                   'Core Objective Functions. I came \\n'\n",
       "                                   'up with them because I wanted to imagine a '\n",
       "                                   'future where we have a trustworthy \\n'\n",
       "                                   'super -intelligence. I asked myself “What '\n",
       "                                   'would it take to have a global AGI that \\n'\n",
       "                                   'is trustworthy and benevolent?” Th e '\n",
       "                                   'answer I came up with was the Core \\n'\n",
       "                                   'Objective Functions.    \\n'\n",
       "                                   '19   Buying In, Doubling Down  \\n'\n",
       "                                   'We indulged in a wonderful flight of fancy '\n",
       "                                   'in the last chapter. We explored'},\n",
       "              'score': 0.8568268,\n",
       "              'sparse_values': {'indices': [], 'values': []},\n",
       "              'values': []},\n",
       "             {'id': '386e6bdc-d56c-4ef4-a1e2-d298b6b708dd',\n",
       "              'metadata': {'chunk': 143.0,\n",
       "                           'text': 'can’t beat a machine that works for free '\n",
       "                                   'and  is ten times faster than any human. \\n'\n",
       "                                   'Not to mention, the AGI is top talent '\n",
       "                                   'since it learned from all the best '\n",
       "                                   'experts!  \\n'\n",
       "                                   'If your competition is doing it, then you '\n",
       "                                   'need to as well.  Adapt or die.  \\n'\n",
       "                                   'The big election debates  roll around and '\n",
       "                                   'the hot -button question comes up: \\n'\n",
       "                                   'what are the candidates going to do about '\n",
       "                                   'AGI? Predictably, the other side isn’t \\n'\n",
       "                                   'going to do anything about it, and just '\n",
       "                                   'pretend like everything is fine. But '\n",
       "                                   'your \\n'\n",
       "                                   'team is all about the AGI! Man, the AGI '\n",
       "                                   'is  the solution to everything! The \\n'\n",
       "                                   'debate stage is set on fire (figuratively) '\n",
       "                                   'when your favorite candidate says that \\n'\n",
       "                                   'they are going start consolidating '\n",
       "                                   'government agencies and replace them '\n",
       "                                   'with \\n'\n",
       "                                   'AGI using the Core Objective Functions. '\n",
       "                                   'What  is the point of gov ernment if \\n'\n",
       "                                   'not to reduce the suffering of its '\n",
       "                                   'citizens? And to increase their '\n",
       "                                   'prosperity? \\n'\n",
       "                                   'Increasing understanding is something that '\n",
       "                                   'everyone agrees the government \\n'\n",
       "                                   'should do a lot more of.  In the meantime, '\n",
       "                                   'your favorite candidate says that we \\n'\n",
       "                                   'need to tap t he brakes and slow down so '\n",
       "                                   'that we have time to adapt. They say \\n'\n",
       "                                   'they want to implement basic income and '\n",
       "                                   'other safety net guarantees to take \\n'\n",
       "                                   'care of the people losing their jobs.  \\n'\n",
       "                                   'The election results come in and your '\n",
       "                                   'candidate wins by a landslide.  \\n'\n",
       "                                   'This is incredible! The government is '\n",
       "                                   'going to start shuttering departments \\n'\n",
       "                                   'in favor of running everything with AGI . '\n",
       "                                   'No more inefficiencies, just high-\\n'\n",
       "                                   'speed  solutions. Within months, for the '\n",
       "                                   'first time in forever, the government is \\n'\n",
       "                                   'running fa ster and cheaper than it was '\n",
       "                                   'last year, and the quality of its services '\n",
       "                                   'is \\n'\n",
       "                                   'going up! Even more shocking, the approval '\n",
       "                                   'ratings of the left and the right are'},\n",
       "              'score': 0.84992415,\n",
       "              'sparse_values': {'indices': [], 'values': []},\n",
       "              'values': []},\n",
       "             {'id': '9b0b34df-ea86-4347-bee4-94b32f50d3b6',\n",
       "              'metadata': {'chunk': 137.0,\n",
       "                           'text': 'intelligent. Now, I don’t mean to diminish '\n",
       "                                   'the value of multimodal input and \\n'\n",
       "                                   'output, I just want to point out that the '\n",
       "                                   'threshold for “true AGI” is low, \\n'\n",
       "                                   'depending on your definition of '\n",
       "                                   'intelligence. As such, I anticip ate that '\n",
       "                                   'we will \\n'\n",
       "                                   'soon have many kinds of  AGI and robots. '\n",
       "                                   'For this book, the definition of AGI \\n'\n",
       "                                   'remains: spontaneous learning and full '\n",
       "                                   'autonomy.  \\n'\n",
       "                                   'Some of our AGI will be strictly chatbots, '\n",
       "                                   'only able to interact via text \\n'\n",
       "                                   'message and apps. Their entire world wi ll '\n",
       "                                   'be text -based, and these kinds of AGI \\n'\n",
       "                                   'will likely exist commercially before '\n",
       "                                   '2025.  I have participated in research '\n",
       "                                   'with \\n'\n",
       "                                   'several groups attempting to bring '\n",
       "                                   'thinking machines to the market. It’s only '\n",
       "                                   'a \\n'\n",
       "                                   'matter of time.  \\n'\n",
       "                                   'What if we want our AGI to be embodied ? '\n",
       "                                   'What if we want android \\n'\n",
       "                                   'companions like Commander Data? That’s '\n",
       "                                   'going to take a bit more time and \\n'\n",
       "                                   'work. The first problem is the hardware, '\n",
       "                                   'which is being worked on by famous \\n'\n",
       "                                   'companies like Honda, Boston Dynamics, and '\n",
       "                                   'now Tesla. Hardware is, \\n'\n",
       "                                   'however, less than half the battle where '\n",
       "                                   'AGI is concerned. Without a powerful \\n'\n",
       "                                   'mind, the best hardware is just a '\n",
       "                                   'glorified mannequin! There are unlimited '\n",
       "                                   'form \\n'\n",
       "                                   'factors we can build for our AGI, from '\n",
       "                                   'cars to drones to human -shaped robots.  \\n'\n",
       "                                   'All of them are useless without th e right '\n",
       "                                   'brain.   \\n'\n",
       "                                   'Since AGI will require more computing '\n",
       "                                   'power than we can stuff into a \\n'\n",
       "                                   'small robot or even a large car, I suspect '\n",
       "                                   'that most AGI will run in the cloud \\n'\n",
       "                                   'and use hardware as peripheral devices, '\n",
       "                                   'like how your computer sees your \\n'\n",
       "                                   'keyboard as an exte rnal device, or your '\n",
       "                                   'phone sees your Bluetooth  speaker as a'},\n",
       "              'score': 0.8492287,\n",
       "              'sparse_values': {'indices': [], 'values': []},\n",
       "              'values': []},\n",
       "             {'id': 'b2311947-b10e-4993-a6f5-2b3880ad7438',\n",
       "              'metadata': {'chunk': 130.0,\n",
       "                           'text': 'of ensuring that an artificial general '\n",
       "                                   'intelligence 130 | P a g e    Benevolent '\n",
       "                                   'by Design  \\n'\n",
       "                                   '(AGI) behaves in  a way that we intend, '\n",
       "                                   'rather than in a \\n'\n",
       "                                   'way that we do not intend. AGI has the '\n",
       "                                   'potential to \\n'\n",
       "                                   'cause tremendous harm if it is not '\n",
       "                                   'designed and \\n'\n",
       "                                   'supervised carefully.  \\n'\n",
       "                                   ' \\n'\n",
       "                                   'This bodes well for us humans! Our '\n",
       "                                   'prototype AGI already understands \\n'\n",
       "                                   'our chief anxiety! Let’s  take this one '\n",
       "                                   'step further:  \\n'\n",
       "                                   ' \\n'\n",
       "                                   'Input: Why are humans anxious about '\n",
       "                                   'AGI?  \\n'\n",
       "                                   'Output: One reason humans are anxious '\n",
       "                                   'about AGI is \\n'\n",
       "                                   'that some people believe that AGI could be '\n",
       "                                   'used to \\n'\n",
       "                                   'create powerful artificial intelligence '\n",
       "                                   'systems that \\n'\n",
       "                                   'could outcompete humans in a n umber of '\n",
       "                                   'domains, \\n'\n",
       "                                   'including economic productivity, strategic '\n",
       "                                   'planning, and \\n'\n",
       "                                   'scientific creativity. AGI could also be '\n",
       "                                   'used to create \\n'\n",
       "                                   'novel and dangerous technologies, or to '\n",
       "                                   'effectively hack \\n'\n",
       "                                   'human societies and economies.  \\n'\n",
       "                                   ' \\n'\n",
       "                                   'I couldn’t have put it better myself ! '\n",
       "                                   'Let’s see what the LLM suggests we do \\n'\n",
       "                                   'to do control AGI.  \\n'\n",
       "                                   ' \\n'\n",
       "                                   'Input: What are some solutions to the '\n",
       "                                   'Control Problem \\n'\n",
       "                                   'of AGI? \\n'\n",
       "                                   'Output: Some possible solutions to the '\n",
       "                                   'control \\n'\n",
       "                                   'problem of AGI include:  \\n'\n",
       "                                   '1. creating \"friendly AI\" – AI that is '\n",
       "                                   'programmed to \\n'\n",
       "                                   'act in accordance with human values  16   '\n",
       "                                   'More Data Is Better    P a g e  | 131 \\n'\n",
       "                                   ' 2. building AI with a \"default governor\" '\n",
       "                                   '– a \\n'\n",
       "                                   'mechanism that ensures the AI behaves in a '\n",
       "                                   'way that is \\n'\n",
       "                                   'aligned with human values, even if it is '\n",
       "                                   'not \\n'\n",
       "                                   'specifically programmed to do so  \\n'\n",
       "                                   '3. creating a \"master AI\" – an AI that is '\n",
       "                                   'respons ible \\n'\n",
       "                                   'for controlling all other AI systems'},\n",
       "              'score': 0.84920394,\n",
       "              'sparse_values': {'indices': [], 'values': []},\n",
       "              'values': []}],\n",
       " 'namespace': ''}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4vO81SccuOp"
   },
   "source": [
    "With Retrieval done we can now work on giving this text to gpt-3.5 to produce answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXoyeckrc6g2"
   },
   "source": [
    "# Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dVbCK1sc7r_"
   },
   "source": [
    "GPT-3.5 is currently accessed via the ChatCompletions endpoint of OpenAI. To add the information we retrieved into the model, we need to pass it into our user prompts alongside our original query. We can do that like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IkCFNpTTc_SQ"
   },
   "outputs": [],
   "source": [
    "# get list of retrieved text\n",
    "contexts = [item['metadata']['text'] for item in res['matches']]\n",
    "\n",
    "augmented_query = \"\\n\\n---\\n\\n\".join(contexts)+\"\\n\\n-----\\n\\n\"+query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HQ5LyunXdC4-",
    "outputId": "cfb1d463-3f7c-4403-f5f0-3e8ca6064e48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Many people I ’ve talked to do not want a super powerful AGI to exist ever. \n",
      "Under no circumstances do they want Skynet to exist, even if it is benevolent by \n",
      "design. While I understand their sentiment, I think we must  accept the \n",
      "possibility that an arms race between nati ons will necessitate the creation of ever -\n",
      "more -powerful AGI systems. Similarly, we would rather live in a world without \n",
      "nuclear weapons or other WMD, but they exist and so we must learn to live \n",
      "with it. In the case of AGI, it can exist and therefore it pro bably will exist, so we \n",
      "must  adapt and do it right . Instead of just tolerating a new technology, it would \n",
      "be best to create a supremely beneficial version of that technology , we don’t \n",
      "want to “just live with AGI,” we want it to make everyone’s life way bet ter. \n",
      "Humans often rail against new technology, especially when they don’t \n",
      "understand it. My Core Objective Functions are no different: most people \n",
      "struggle to understand what they are and how they work (another reason I’m \n",
      "writing this book!) Suspicion of n ew technologies is healthy and necessary, and I \n",
      "welcome it. Look at plastics, which have been poisoning us with parabens and \n",
      "phthalates for many years. Even the most innocuous inventions can have 152 | P a g e    Benevolent by Design  \n",
      "profound repercussions unless we fully understand them, and they earn our \n",
      "trust over time. Other technologies, like Soviet -style communism, have rightly \n",
      "earned our mistrust  because that invention turned out to be incredibly harmful.  \n",
      "Trust is the first flaw of my Core Objective Functions. Hold the AGI\n",
      "\n",
      "---\n",
      "\n",
      "couple years ago, and over time, you save up enough money to get your first \n",
      "mini-yacht.  \n",
      "All the while, the AGIs have gotten faster and smarter, and all health issues \n",
      "are history. Cancer? Gone. Coronavirus? Man, who even remembers that? Heart \n",
      "disease? We got a vaccine for that. Dementia? Well, no one can help your in-\n",
      "laws with that, but that’s beside the point.  \n",
      "We have now arrived at the singularity, the point in time where all these \n",
      "compounding returns transform human existence into something completely \n",
      "novel, that we can’t even yet imagine. A benevolent AGI must be part of this  \n",
      "outcome . \n",
      "To take this one step further, let’s imagine that every nation on th e planet \n",
      "implements AGI government. The AGI is smarter, fairer, and kinder than any \n",
      "human leaders. Certainly, we still want humans in the loop, but most \n",
      "government functions can be replaced by AGI. Over many decades and \n",
      "centuries, the AGI helps humanity to  understand each other and achieve an \n",
      "incomprehensible level of peace and prosperity. Future generations grow up \n",
      "with AGI, like how children are growing up with Alexa today. Global unity is \n",
      "achieved with the help of the Core Objective Functions.  \n",
      "This utopi an vision is my goal with the Core Objective Functions. I came \n",
      "up with them because I wanted to imagine a future where we have a trustworthy \n",
      "super -intelligence. I asked myself “What would it take to have a global AGI that \n",
      "is trustworthy and benevolent?” Th e answer I came up with was the Core \n",
      "Objective Functions.    \n",
      "19   Buying In, Doubling Down  \n",
      "We indulged in a wonderful flight of fancy in the last chapter. We explored\n",
      "\n",
      "---\n",
      "\n",
      "can’t beat a machine that works for free and  is ten times faster than any human. \n",
      "Not to mention, the AGI is top talent since it learned from all the best experts!  \n",
      "If your competition is doing it, then you need to as well.  Adapt or die.  \n",
      "The big election debates  roll around and the hot -button question comes up: \n",
      "what are the candidates going to do about AGI? Predictably, the other side isn’t \n",
      "going to do anything about it, and just pretend like everything is fine. But your \n",
      "team is all about the AGI! Man, the AGI is  the solution to everything! The \n",
      "debate stage is set on fire (figuratively) when your favorite candidate says that \n",
      "they are going start consolidating government agencies and replace them with \n",
      "AGI using the Core Objective Functions. What  is the point of gov ernment if \n",
      "not to reduce the suffering of its citizens? And to increase their prosperity? \n",
      "Increasing understanding is something that everyone agrees the government \n",
      "should do a lot more of.  In the meantime, your favorite candidate says that we \n",
      "need to tap t he brakes and slow down so that we have time to adapt. They say \n",
      "they want to implement basic income and other safety net guarantees to take \n",
      "care of the people losing their jobs.  \n",
      "The election results come in and your candidate wins by a landslide.  \n",
      "This is incredible! The government is going to start shuttering departments \n",
      "in favor of running everything with AGI . No more inefficiencies, just high-\n",
      "speed  solutions. Within months, for the first time in forever, the government is \n",
      "running fa ster and cheaper than it was last year, and the quality of its services is \n",
      "going up! Even more shocking, the approval ratings of the left and the right are\n",
      "\n",
      "---\n",
      "\n",
      "intelligent. Now, I don’t mean to diminish the value of multimodal input and \n",
      "output, I just want to point out that the threshold for “true AGI” is low, \n",
      "depending on your definition of intelligence. As such, I anticip ate that we will \n",
      "soon have many kinds of  AGI and robots. For this book, the definition of AGI \n",
      "remains: spontaneous learning and full autonomy.  \n",
      "Some of our AGI will be strictly chatbots, only able to interact via text \n",
      "message and apps. Their entire world wi ll be text -based, and these kinds of AGI \n",
      "will likely exist commercially before 2025.  I have participated in research with \n",
      "several groups attempting to bring thinking machines to the market. It’s only a \n",
      "matter of time.  \n",
      "What if we want our AGI to be embodied ? What if we want android \n",
      "companions like Commander Data? That’s going to take a bit more time and \n",
      "work. The first problem is the hardware, which is being worked on by famous \n",
      "companies like Honda, Boston Dynamics, and now Tesla. Hardware is, \n",
      "however, less than half the battle where AGI is concerned. Without a powerful \n",
      "mind, the best hardware is just a glorified mannequin! There are unlimited form \n",
      "factors we can build for our AGI, from cars to drones to human -shaped robots.  \n",
      "All of them are useless without th e right brain.   \n",
      "Since AGI will require more computing power than we can stuff into a \n",
      "small robot or even a large car, I suspect that most AGI will run in the cloud \n",
      "and use hardware as peripheral devices, like how your computer sees your \n",
      "keyboard as an exte rnal device, or your phone sees your Bluetooth  speaker as a\n",
      "\n",
      "---\n",
      "\n",
      "of ensuring that an artificial general intelligence 130 | P a g e    Benevolent by Design  \n",
      "(AGI) behaves in  a way that we intend, rather than in a \n",
      "way that we do not intend. AGI has the potential to \n",
      "cause tremendous harm if it is not designed and \n",
      "supervised carefully.  \n",
      " \n",
      "This bodes well for us humans! Our prototype AGI already understands \n",
      "our chief anxiety! Let’s  take this one step further:  \n",
      " \n",
      "Input: Why are humans anxious about AGI?  \n",
      "Output: One reason humans are anxious about AGI is \n",
      "that some people believe that AGI could be used to \n",
      "create powerful artificial intelligence systems that \n",
      "could outcompete humans in a n umber of domains, \n",
      "including economic productivity, strategic planning, and \n",
      "scientific creativity. AGI could also be used to create \n",
      "novel and dangerous technologies, or to effectively hack \n",
      "human societies and economies.  \n",
      " \n",
      "I couldn’t have put it better myself ! Let’s see what the LLM suggests we do \n",
      "to do control AGI.  \n",
      " \n",
      "Input: What are some solutions to the Control Problem \n",
      "of AGI? \n",
      "Output: Some possible solutions to the control \n",
      "problem of AGI include:  \n",
      "1. creating \"friendly AI\" – AI that is programmed to \n",
      "act in accordance with human values  16   More Data Is Better    P a g e  | 131 \n",
      " 2. building AI with a \"default governor\" – a \n",
      "mechanism that ensures the AI behaves in a way that is \n",
      "aligned with human values, even if it is not \n",
      "specifically programmed to do so  \n",
      "3. creating a \"master AI\" – an AI that is respons ible \n",
      "for controlling all other AI systems\n",
      "\n",
      "-----\n",
      "\n",
      "What is the most likely outcome with the invention of AGI\n"
     ]
    }
   ],
   "source": [
    "print(augmented_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAkVhlTQdQ6E"
   },
   "source": [
    "Now we ask a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j_q9bkERdUU3"
   },
   "outputs": [],
   "source": [
    "# system message to 'prime' the model\n",
    "primer = f\"\"\"You are Q&A bot. A highly intelligent system that answers\n",
    "user questions based on the information provided by the user above\n",
    "each question. If the information can not be found in the information\n",
    "provided by the user you truthfully say \"I don't know\".\n",
    "\"\"\"\n",
    "\n",
    "res = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": primer},\n",
    "        {\"role\": \"user\", \"content\": augmented_query}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0R572RddedR"
   },
   "source": [
    "To display this response nicely, we will display it in markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98
    },
    "id": "N5txgvo5df0H",
    "outputId": "4573b9b8-bb6b-4729-b72b-9302c52fdaaf"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "It is difficult to predict the most likely outcome with the invention of AGI as it depends on how the technology is developed, implemented, and controlled. However, some people fear that AGI could potentially cause harm to humanity if not designed and supervised carefully, while others believe that a benevolent AGI could greatly benefit society and even help solve some of humanity's most pressing problems. Ultimately, the outcome will depend on how we adapt and control this technology."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "display(Markdown(res['choices'][0]['message']['content']))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nG7-U4xC4rcE"
   },
   "source": [
    "Changing notebook, so just reinitializing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y338927dkxlY"
   },
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,  # app.pinecone.io\n",
    "    environment=PINECONE_ENV  # next to API key in console\n",
    ")\n",
    "\n",
    "index_name = \"ask-ben\"\n",
    "\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    raise ValueError(\n",
    "        f\"No '{index_name}' index exists. You must create the index before \"\n",
    "        \"running this notebook. Please refer to the walkthrough at \"\n",
    "        \"'github.com/pinecone-io/examples'.\"  # TODO add full link\n",
    "    )\n",
    "\n",
    "index = pinecone.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tPQrkvaclBrD",
    "outputId": "98a4468e-30b2-4f95-dbf6-a32d4f48ab10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': '059b464c-c087-4411-b55e-b330171f69e0',\n",
       "              'metadata': {'chunk': 144.0,\n",
       "                           'text': 'microphone, speaker, and screen. The '\n",
       "                                   'camera and microphone would be used \\n'\n",
       "                                   'to generate the contexts w hile the '\n",
       "                                   'speaker and screen would be used for '\n",
       "                                   'output.  \\n'\n",
       "                                   'What could a home NLCA do?  \\n'\n",
       "                                   'At first, it would just provide good '\n",
       "                                   'conversation – a few steps above current \\n'\n",
       "                                   'smart home devices. You could talk to your '\n",
       "                                   'NLCA home device about your life, \\n'\n",
       "                                   'work, family, frie nds, and anything you '\n",
       "                                   'are curious about. Just look at the '\n",
       "                                   'level \\n'\n",
       "                                   'of conversation NLCA is already capable of '\n",
       "                                   'in Appendix C and add a voice \\n'\n",
       "                                   'interface.  \\n'\n",
       "                                   'In the future, NLCA could be given a '\n",
       "                                   'robotic chassis so that it can assist '\n",
       "                                   'with \\n'\n",
       "                                   'household chores like cooki ng and '\n",
       "                                   'cleaning, or even childcare.  \\n'\n",
       "                                   'Mobile  \\n'\n",
       "                                   'As with home devices, it will be several '\n",
       "                                   'decades before we can cram enough \\n'\n",
       "                                   'computational power into cars and mobile '\n",
       "                                   'phones  to run NLCA without an \\n'\n",
       "                                   'internet connection. Mobile computing is '\n",
       "                                   '30 years behind supercomp uters in \\n'\n",
       "                                   'terms of processing power. Since '\n",
       "                                   'transformers require huge server clusters '\n",
       "                                   'to 158 | P a g e    Natural Language '\n",
       "                                   'Cognitive Architecture  \\n'\n",
       "                                   ' run today, it will be twenty to thirty '\n",
       "                                   'years before you could run NLCA on your \\n'\n",
       "                                   'smartphone . As such, any portable '\n",
       "                                   'instances of NLCA will require an '\n",
       "                                   'internet \\n'\n",
       "                                   'connection to  function.  \\n'\n",
       "                                   'A fully functional information concierge '\n",
       "                                   'will need to be portable . While we \\n'\n",
       "                                   'do spend much of our time at home, some of '\n",
       "                                   'our most important activities and \\n'\n",
       "                                   'life events take place outside of the '\n",
       "                                   'home.  As with the home device, which  '\n",
       "                                   'can \\n'\n",
       "                                   'learn all ab out your needs and '\n",
       "                                   'preferences, the mobile version can assist '\n",
       "                                   'while \\n'\n",
       "                                   'you are out and about.  \\n'\n",
       "                                   'What could a mobile NLCA do for you?  \\n'\n",
       "                                   'One of the most popular tropes in science '\n",
       "                                   'fiction today is the AI companion.'},\n",
       "              'score': 0.0,\n",
       "              'values': []}],\n",
       " 'namespace': ''}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.query([0.0]*1536, top_k=1, include_metadata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFTiO7eX2mkN"
   },
   "source": [
    "Initialize the retrieval components (embedding model and vector DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GPvqFFGXcFGF"
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "vectordb = Pinecone(\n",
    "    index=index,\n",
    "    embedding_function=embeddings.embed_query,\n",
    "    text_key=\"text\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPFXYJE92mNm"
   },
   "source": [
    "Initialize `gpt-3.5-turbo` chat model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jESRtKeJk1IO"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    temperature=0,\n",
    "    model_name='gpt-3.5-turbo'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7isHNe1Gtp_"
   },
   "source": [
    "We then initialize the QA retrieval object using our `llm` and the `vectordb.as_retriever()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PkTHIAE47uJp"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "retriever = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectordb.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtaTKxTJG7J8"
   },
   "source": [
    "One additional thing we have here is the `chain_type=\"stuff\"`. There are two options here, `\"stuff\"` or `\"map_reduce\"`. The `map_reduce` option essentially summarizes returned documents, whereas the `stuff` option just returns the retrieved documents as is.\n",
    "\n",
    "The `retriever` is ready and can be used by us like this. However, we need to convert it into a `Tool` to be used by our conversational agent. To do that we need the `retriever` itself, a tool description, and a tool name. We use these to initialize the tool like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rEQEdRQRmCbO"
   },
   "outputs": [],
   "source": [
    "tool_desc = \"\"\"Use this tool to answer user questions using book knowledgebase. If the user states 'ask Book' use this tool to get\n",
    "the answer. This tool can also be used for follow up questions from\n",
    "the user.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "abClSi8GlC8O"
   },
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "\n",
    "tools = [Tool(\n",
    "    func=retriever.run,\n",
    "    description=tool_desc,\n",
    "    name='Book'\n",
    ")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9o6IIHSHwVX"
   },
   "source": [
    "With that, we're ready to initialize the conversational agent. As it is a *conversational* agent, it does need some form of [conversational memory](https://www.pinecone.io/learn/langchain-conversational-memory/). For this we will use the `ConversationBufferWindowMemory` option, which will *remember* the previous `k` interactions between the user and the AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_CRqHPrmcHo"
   },
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    memory_key=\"chat_history\",  # important to align with agent prompt (below)\n",
    "    k=5,\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ArdEDGH7nBRR"
   },
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "\n",
    "conversational_agent = initialize_agent(\n",
    "    agent='chat-conversational-react-description', \n",
    "    tools=tools, \n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    max_iterations=2,\n",
    "    early_stopping_method=\"generate\",\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8EkS7lG3ElB"
   },
   "source": [
    "Important items in `agent` parameter:\n",
    "\n",
    "* `chat-conversational`: for chatbots with conversational memory.\n",
    "* `react`: refers to the ReAct framework.\n",
    "* `description`: because the LLM relies on the tool description to decide which tool to use.\n",
    "\n",
    "### Conversational Agent Prompt\n",
    "\n",
    "The prompt of the conversational agent is fairly complex. Let's create it then break it down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m1ApK_bkBOQ7",
    "outputId": "b08386e7-ea60-465c-995d-5f8403aae2b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input', 'chat_history', 'agent_scratchpad'], output_parser=None, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template='Assistant is a large language model trained by OpenAI.\\n\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.', template_format='f-string', validate_template=True), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, template='TOOLS\\n------\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\n\\n> Book: Use this tool to answer user questions using book knowledgebase. If the user states \\'ask Book\\' use this tool to get\\nthe answer. This tool can also be used for follow up questions from\\nthe user.\\n\\nRESPONSE FORMAT INSTRUCTIONS\\n----------------------------\\n\\nWhen responding to me please, please output a response in one of two formats:\\n\\n**Option 1:**\\nUse this if you want the human to use a tool.\\nMarkdown code snippet formatted in the following schema:\\n\\n```json\\n{{\\n    \"action\": string \\\\ The action to take. Must be one of Book\\n    \"action_input\": string \\\\ The input to the action\\n}}\\n```\\n\\n**Option #2:**\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\n\\n```json\\n{{\\n    \"action\": \"Final Answer\",\\n    \"action_input\": string \\\\ You should put what you want to return to use here\\n}}\\n```\\n\\nUSER\\'S INPUT\\n--------------------\\nHere is the user\\'s input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\n\\n{input}', template_format='f-string', validate_template=True), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_agent.agent.llm_chain.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISOscagrnDyD"
   },
   "outputs": [],
   "source": [
    "sys_msg = \"\"\"You are a helpful chatbot that answers the user's questions.\n",
    "\"\"\"\n",
    "\n",
    "prompt = conversational_agent.agent.create_prompt(\n",
    "    system_message=sys_msg,\n",
    "    tools=tools\n",
    ")\n",
    "conversational_agent.agent.llm_chain.prompt = prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ezpd5vaOMC-w"
   },
   "source": [
    "We can see the prompt template like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lvEAE9XKMFIR",
    "outputId": "2c923a24-d70b-48d6-c50a-408d5510f938"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input', 'chat_history', 'agent_scratchpad'], output_parser=None, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template=\"You are a helpful chatbot that answers the user's questions.\\n\", template_format='f-string', validate_template=True), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, template='TOOLS\\n------\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\n\\n> Book: Use this tool to answer user questions using book knowledgebase. If the user states \\'ask Book\\' use this tool to get\\nthe answer. This tool can also be used for follow up questions from\\nthe user.\\n\\nRESPONSE FORMAT INSTRUCTIONS\\n----------------------------\\n\\nWhen responding to me please, please output a response in one of two formats:\\n\\n**Option 1:**\\nUse this if you want the human to use a tool.\\nMarkdown code snippet formatted in the following schema:\\n\\n```json\\n{{\\n    \"action\": string \\\\ The action to take. Must be one of Book\\n    \"action_input\": string \\\\ The input to the action\\n}}\\n```\\n\\n**Option #2:**\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\n\\n```json\\n{{\\n    \"action\": \"Final Answer\",\\n    \"action_input\": string \\\\ You should put what you want to return to use here\\n}}\\n```\\n\\nUSER\\'S INPUT\\n--------------------\\nHere is the user\\'s input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\n\\n{input}', template_format='f-string', validate_template=True), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_agent.agent.llm_chain.prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56MZZYTTs0Jq"
   },
   "source": [
    "The conversational agent prompt is defined by the `ChatPromptTemplate`. Let's break it down:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ybUWUOZOFAL",
    "outputId": "f570cbcd-94c7-4638-abc5-599b98fe3884"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input', 'chat_history', 'agent_scratchpad']"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_agent.agent.llm_chain.prompt.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axHAsgRROFUO"
   },
   "source": [
    " This prompt template contains *three* `input_variables`, those are:\n",
    "\n",
    "* `input`: The new user input to the chatbot, i.e. our prompt/query.\n",
    "\n",
    "* `chat_history`: We defined this above in the `ConversationBufferWindowMemory` definition.\n",
    "\n",
    "* `agent_scratchpad`: This is where we store the thoughts of the LLM as it is deciding which tools to interact with and *how* to interact with them.\n",
    "\n",
    "These `input_variables` are fed into the `messages` contained within the prompt template, let's see what we have there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-3z2E1rlOIr_",
    "outputId": "afbaf923-b218-4770-bb5e-03a9da7e33a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template=\"You are a helpful chatbot that answers the user's questions.\\n\", template_format='f-string', validate_template=True), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='chat_history'),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, template='TOOLS\\n------\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\n\\n> Book: Use this tool to answer user questions using book knowledgebase. If the user states \\'ask Book\\' use this tool to get\\nthe answer. This tool can also be used for follow up questions from\\nthe user.\\n\\nRESPONSE FORMAT INSTRUCTIONS\\n----------------------------\\n\\nWhen responding to me please, please output a response in one of two formats:\\n\\n**Option 1:**\\nUse this if you want the human to use a tool.\\nMarkdown code snippet formatted in the following schema:\\n\\n```json\\n{{\\n    \"action\": string \\\\ The action to take. Must be one of Book\\n    \"action_input\": string \\\\ The input to the action\\n}}\\n```\\n\\n**Option #2:**\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\n\\n```json\\n{{\\n    \"action\": \"Final Answer\",\\n    \"action_input\": string \\\\ You should put what you want to return to use here\\n}}\\n```\\n\\nUSER\\'S INPUT\\n--------------------\\nHere is the user\\'s input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\n\\n{input}', template_format='f-string', validate_template=True), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_agent.agent.llm_chain.prompt.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuMfxYayOIbu"
   },
   "source": [
    "\n",
    "It's a little hard to see here, but there are **three** components in `messages`. Those are:\n",
    "\n",
    "* `SystemMessagePromptTemplate`\n",
    "\n",
    "* `MessagesPlaceholder`\n",
    "\n",
    "* `HumanMessagePromptTemplate`\n",
    "\n",
    "Let's start with the first item, the `SystemMessage`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D9JWEqIzs9oC",
    "outputId": "9a786160-bcc3-4637-9a6b-faeaf1de4eb9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template=\"You are a helpful chatbot that answers the user's questions.\\n\", template_format='f-string', validate_template=True), additional_kwargs={})"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_agent.agent.llm_chain.prompt.messages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b2MkeX4_tIga",
    "outputId": "a0f0284f-b5ce-4f3e-b963-aa44b1366c36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful chatbot that answers the user's questions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    conversational_agent.agent.llm_chain.prompt.messages[0].prompt.template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pill7M9AtdIJ"
   },
   "source": [
    "That is our initial system message that we set earlier with the `sys_msg`. There's not much to say about this other than it is used to \"prime\" (set the initial objective of) the model.\n",
    "\n",
    "Next we have the `MessagesPlaceholder`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vHeZW75itky1",
    "outputId": "2a280b89-3882-4a01-a1ca-a6e6ffb419b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MessagesPlaceholder(variable_name='chat_history')"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_agent.agent.llm_chain.prompt.messages[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89ktagFTSkvv"
   },
   "source": [
    "We can see from `'chat_history'` (this must align to the `memory_key` from the `ConversationBufferWindowMemory` initialized earlier) that this is where the previous messages of the conversation will be fed into the LLM.\n",
    "\n",
    "The format of this input is set by the type of conversational memory being used, which in this case is the `ConversationBufferWindowMemory`.\n",
    "\n",
    "Finally, we have the `HumanMessagePromptTemplate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fcIVdeklTZwh",
    "outputId": "061bcb00-e654-4efd-b750-ea31ef5829c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, template='TOOLS\\n------\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\n\\n> Book: Use this tool to answer user questions using book knowledgebase. If the user states \\'ask Book\\' use this tool to get\\nthe answer. This tool can also be used for follow up questions from\\nthe user.\\n\\nRESPONSE FORMAT INSTRUCTIONS\\n----------------------------\\n\\nWhen responding to me please, please output a response in one of two formats:\\n\\n**Option 1:**\\nUse this if you want the human to use a tool.\\nMarkdown code snippet formatted in the following schema:\\n\\n```json\\n{{\\n    \"action\": string \\\\ The action to take. Must be one of Book\\n    \"action_input\": string \\\\ The input to the action\\n}}\\n```\\n\\n**Option #2:**\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\n\\n```json\\n{{\\n    \"action\": \"Final Answer\",\\n    \"action_input\": string \\\\ You should put what you want to return to use here\\n}}\\n```\\n\\nUSER\\'S INPUT\\n--------------------\\nHere is the user\\'s input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\n\\n{input}', template_format='f-string', validate_template=True), additional_kwargs={})"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_agent.agent.llm_chain.prompt.messages[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NRuqI-sCTZsb",
    "outputId": "cc9880f2-4e45-460b-f449-347d7590812d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOLS\n",
      "------\n",
      "Assistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\n",
      "\n",
      "> Book: Use this tool to answer user questions using book knowledgebase. If the user states 'ask Book' use this tool to get\n",
      "the answer. This tool can also be used for follow up questions from\n",
      "the user.\n",
      "\n",
      "RESPONSE FORMAT INSTRUCTIONS\n",
      "----------------------------\n",
      "\n",
      "When responding to me please, please output a response in one of two formats:\n",
      "\n",
      "**Option 1:**\n",
      "Use this if you want the human to use a tool.\n",
      "Markdown code snippet formatted in the following schema:\n",
      "\n",
      "```json\n",
      "{{\n",
      "    \"action\": string \\ The action to take. Must be one of Book\n",
      "    \"action_input\": string \\ The input to the action\n",
      "}}\n",
      "```\n",
      "\n",
      "**Option #2:**\n",
      "Use this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\n",
      "\n",
      "```json\n",
      "{{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": string \\ You should put what you want to return to use here\n",
      "}}\n",
      "```\n",
      "\n",
      "USER'S INPUT\n",
      "--------------------\n",
      "Here is the user's input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\n",
      "\n",
      "{input}\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    conversational_agent.agent.llm_chain.prompt.messages[2].prompt.template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3NhHdrMTia2"
   },
   "source": [
    "This is the most interesting component. First, we have a single `input` — the user's query/prompt. But before this we see a lot of text, the majority of this is the setup for the LLM to be able to use any tools that we've passed to the conversational agent.\n",
    "\n",
    "In our case, there is just one tool, the `Lex Fridman DB` tool that we defined earlier. We can also see the tool description that we defined. The LLM will use this tool description to figure out which tool (if any) it should use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yo3nDYEWUQLZ"
   },
   "source": [
    "## Having a Conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyiFqLg5UPyM"
   },
   "source": [
    "Let's begin our conversation. We'll start as any typical conversation begins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SwjREzM8nJsx",
    "outputId": "9c5b2edd-bad9-4de3-851d-cb3ca947f4bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"I'm just a chatbot, so I don't have feelings, but I'm here to help you with any questions you have!\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'hi how are you',\n",
       " 'chat_history': [],\n",
       " 'output': \"I'm just a chatbot, so I don't have feelings, but I'm here to help you with any questions you have!\"}"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_agent(\"hi how are you\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sGM7i372Ua7X"
   },
   "source": [
    "Looks good. We should note that there is this **AgentExecutor chain** thing. Where we can see an `\"action\"` and an `\"action_input\"`. It is here where the agent is deciding whether it should use a tool.\n",
    "\n",
    "Here we see the agent decides on `\"action\": \"Final Answer\"`, meaning no tool is required. Therefore, it just uses the LLM as per usual to generate an answer. That answer can be seen in `\"I'm just a chatbot, I don't have feelings, but thanks for asking! How can I assist you today?\"`.\n",
    "\n",
    "What if we mention the words `\"ask Ben\"`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bkgKG1x3ndBJ",
    "outputId": "a170cfb2-cf48-41df-ce02-fa8d6d130353"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mHere's the response using the Book tool:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"action\": \"Book\",\n",
      "    \"action_input\": \"What is the future of AGI?\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe author of the text believes that the future of AGI (Artificial General Intelligence) is a utopian vision where every nation on the planet implements AGI government, which is smarter, fairer, and kinder than any human leaders. The AGI helps humanity to understand each other and achieve an incomprehensible level of peace and prosperity. Future generations grow up with AGI, and global unity is achieved with the help of the Core Objective Functions. The author also believes that more data is better, and the more data we give neural networks, the more powerful, flexible, and robust they become. However, the author acknowledges that some people do not want a super powerful AGI to exist ever, but he thinks that we must accept the possibility that an arms race between nations will necessitate the creation of ever-more-powerful AGI systems.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mSure, here's the response to your previous question:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The author of the text believes that the future of AGI is a utopian vision where every nation on the planet implements AGI government, which is smarter, fairer, and kinder than any human leaders. The AGI helps humanity to understand each other and achieve an incomprehensible level of peace and prosperity. Future generations grow up with AGI, and global unity is achieved with the help of the Core Objective Functions. The author also believes that more data is better, and the more data we give neural networks, the more powerful, flexible, and robust they become. However, the author acknowledges that some people do not want a super powerful AGI to exist ever, but he thinks that we must accept the possibility that an arms race between nations will necessitate the creation of ever-more-powerful AGI systems.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'ask Book about the future of AGI',\n",
       " 'chat_history': [HumanMessage(content='hi how are you', additional_kwargs={}),\n",
       "  AIMessage(content=\"I'm just a chatbot, so I don't have feelings, but I'm here to help you with any questions you have!\", additional_kwargs={}),\n",
       "  HumanMessage(content='ask Book about the future of AGI', additional_kwargs={}),\n",
       "  AIMessage(content=\"I'm sorry, but I don't have any record of your last comment. Could you please repeat it?\", additional_kwargs={})],\n",
       " 'output': 'The author of the text believes that the future of AGI is a utopian vision where every nation on the planet implements AGI government, which is smarter, fairer, and kinder than any human leaders. The AGI helps humanity to understand each other and achieve an incomprehensible level of peace and prosperity. Future generations grow up with AGI, and global unity is achieved with the help of the Core Objective Functions. The author also believes that more data is better, and the more data we give neural networks, the more powerful, flexible, and robust they become. However, the author acknowledges that some people do not want a super powerful AGI to exist ever, but he thinks that we must accept the possibility that an arms race between nations will necessitate the creation of ever-more-powerful AGI systems.'}"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_agent(\"ask Book about the future of AGI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OvmK5lnzDgU4",
    "outputId": "3c0f9ba3-09c9-4ce2-c16f-bb630b592689"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Book\",\n",
      "    \"action_input\": \"What does the book say about space exploration?\"\n",
      "}\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe book mentions space exploration as one of the significant advancements in science and technology. It states that we are preparing to fly to other planets for the first time, thus spreading our DNA across the cosmos. If we succeed in becoming a multiplanetary species, then the evolutionary gambit of big brains will have paid off. The book also suggests that the search for extraterrestrial life can be seen as a response to cosmic abandonment, as the possibility of discovering other intelligent beings offers the hope of connection and a sense of belonging in the vastness of the universe.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The book mentions space exploration as a significant advancement in science and technology. It suggests that becoming a multiplanetary species is a significant evolutionary achievement, and the search for extraterrestrial life can offer hope for connection and a sense of belonging in the vastness of the universe.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what does the book think about space exploration?',\n",
       " 'chat_history': [HumanMessage(content='hi how are you', additional_kwargs={}),\n",
       "  AIMessage(content=\"I'm just a chatbot, so I don't have feelings, but I'm here to help you with any questions you have!\", additional_kwargs={}),\n",
       "  HumanMessage(content='ask Book about the future of AGI', additional_kwargs={}),\n",
       "  AIMessage(content=\"I'm sorry, but I don't have any record of your last comment. Could you please repeat it?\", additional_kwargs={}),\n",
       "  HumanMessage(content='ask Book about the future of AGI', additional_kwargs={}),\n",
       "  AIMessage(content='The author of the text believes that the future of AGI is a utopian vision where every nation on the planet implements AGI government, which is smarter, fairer, and kinder than any human leaders. The AGI helps humanity to understand each other and achieve an incomprehensible level of peace and prosperity. Future generations grow up with AGI, and global unity is achieved with the help of the Core Objective Functions. The author also believes that more data is better, and the more data we give neural networks, the more powerful, flexible, and robust they become. However, the author acknowledges that some people do not want a super powerful AGI to exist ever, but he thinks that we must accept the possibility that an arms race between nations will necessitate the creation of ever-more-powerful AGI systems.', additional_kwargs={})],\n",
       " 'output': 'The book mentions space exploration as a significant advancement in science and technology. It suggests that becoming a multiplanetary species is a significant evolutionary achievement, and the search for extraterrestrial life can offer hope for connection and a sense of belonging in the vastness of the universe.'}"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_agent(\"what does the book think about space exploration?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FOV1l6plD249",
    "outputId": "539dc28c-13b5-4721-9792-d4293e9bdfc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Lex Fridman DB\",\n",
      "    \"action_input\": \"What is the future of AI? Please provide the title and URL of the source if possible.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe context provided includes multiple conversations about the future of AI, but there is no single source or article that provides a definitive answer to this question. The opinions and speculations about the future of AI vary widely depending on who you ask and what their expertise is. Some people are optimistic about the potential benefits of AI for humanity, while others are concerned about the risks and potential dangers. If you have a specific aspect of the future of AI that you are interested in, I can try to provide more information based on the context provided.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"There is no single source or article that provides a definitive answer to the question of the future of AI. The opinions and speculations about the future of AI vary widely depending on who you ask and what their expertise is. Some people are optimistic about the potential benefits of AI for humanity, while others are concerned about the risks and potential dangers. If you have a specific aspect of the future of AI that you are interested in, I can try to provide more information based on the context provided.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'ask lex about the future of AI, include the title and url of this information if possible',\n",
       " 'chat_history': [HumanMessage(content='hi how are you', additional_kwargs={}),\n",
       "  AIMessage(content=\"I'm just a chatbot, so I don't have feelings, but I'm here to help you with any questions you have!\", additional_kwargs={}),\n",
       "  HumanMessage(content='hi how are you', additional_kwargs={}),\n",
       "  AIMessage(content=\"I'm just a chatbot, so I don't have feelings, but I'm here to help you with any questions you have!\", additional_kwargs={}),\n",
       "  HumanMessage(content='ask lex about the future of ai', additional_kwargs={}),\n",
       "  AIMessage(content='Lex Fridman discussed the potential of AI to increase the quality of life, cure diseases, increase material wealth, and help people be happier and more fulfilled. He also mentioned the need for AI to be aligned with humans and not hurt or limit them. Additionally, he talked about the possibility of AI systems exhibiting human-like emotions and feelings, which would require difficult discussions about how to treat them and what role they would have in society. Finally, he speculated about the future of humans and robots in a multi-planetary species world, hoping for a one-to-one ratio and emphasizing the importance of humans being part of the equation.', additional_kwargs={}),\n",
       "  HumanMessage(content='what does he think about space exploration?', additional_kwargs={}),\n",
       "  AIMessage(content='Lex Fridman is very enthusiastic about space exploration and believes that it is one of the most inspiring things humans can do. He thinks that reaching out into the unknown inspires people on Earth and can also inspire a new generation of scientists. He also believes that the current explosion of new capabilities in the space industry is driving incredible innovation and making it a great time to be in space.', additional_kwargs={}),\n",
       "  HumanMessage(content='what are the titles and sources for this information?', additional_kwargs={}),\n",
       "  AIMessage(content=\"I apologize for the confusion. The information I provided earlier was based on my programming and knowledge base. I do not have a specific source for the information. However, you can find more information on the topics I discussed by doing a quick search on the internet or by listening to Lex Fridman's podcasts.\", additional_kwargs={})],\n",
       " 'output': 'There is no single source or article that provides a definitive answer to the question of the future of AI. The opinions and speculations about the future of AI vary widely depending on who you ask and what their expertise is. Some people are optimistic about the potential benefits of AI for humanity, while others are concerned about the risks and potential dangers. If you have a specific aspect of the future of AI that you are interested in, I can try to provide more information based on the context provided.'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_agent(\"ask lex about the future of AI, include the title and url of this information if possible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aL5fojo9D2yh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-dcGpHZSXs2Z"
   },
   "source": [
    "Great, we can see that the first thing the agent did was default to the `\"Lex Fridman DB\"` tool. The input to that tool was generated by the LLM, and is `\"What did Lex Fridman say about the future of AI?\"`.\n",
    "\n",
    "This input is then passed into the `Lex Fridman DB` tool and the output observation of the LLM (after it has read all of the information returned by our vector DB is returned to our agent. From this observation the agent moves on to the `\"Final Answer\"` action, giving us the output."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07b62ec44bb64d94b04859a54307167b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20483e8d037b49c295c185149e2ba2ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_07b62ec44bb64d94b04859a54307167b",
      "placeholder": "​",
      "style": "IPY_MODEL_983a80897b844c85b4bc118d81c6617f",
      "value": " 4/4 [00:00&lt;00:00,  4.26it/s]"
     }
    },
    "28c4a0baf51f4cdb8718ada1c518b929": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2b4b7dbb07a34daa8951422fe9237cfc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4d017abbab6a4607b8c98c1874b14fdb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae05a5fda3364eeab7a9a5a233df299d",
      "max": 7,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b60d5e513c9649ccab44969745151342",
      "value": 7
     }
    },
    "4e207721fd914c30bd176393a3b43360": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b7f761f3cf5542ad89eac15a40dfab43",
      "placeholder": "​",
      "style": "IPY_MODEL_d7a76b188cf94565bc760b275c24c72b",
      "value": "100%"
     }
    },
    "5e13d888ee6b40f18f8fcf7530a6f703": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_85fbc34b2cf643739490adaa6098add1",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8ff451546dab4413a22926c1d0ef626b",
      "value": 4
     }
    },
    "71b1c2ce3c014048b3eb351ef9d2b0b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7b9331b9c35142f396c8df6b2fc69142",
       "IPY_MODEL_5e13d888ee6b40f18f8fcf7530a6f703",
       "IPY_MODEL_20483e8d037b49c295c185149e2ba2ce"
      ],
      "layout": "IPY_MODEL_ce0640b089774bebbb485895abb5b6a0"
     }
    },
    "7759081e382942ba91fa419b246e9f38": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_908ceb3775984baa9f462a6db7611c7e",
      "placeholder": "​",
      "style": "IPY_MODEL_2b4b7dbb07a34daa8951422fe9237cfc",
      "value": " 7/7 [00:07&lt;00:00,  1.09it/s]"
     }
    },
    "7b9331b9c35142f396c8df6b2fc69142": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_98fd3d2619704051812999c9b00afae5",
      "placeholder": "​",
      "style": "IPY_MODEL_28c4a0baf51f4cdb8718ada1c518b929",
      "value": "100%"
     }
    },
    "85fbc34b2cf643739490adaa6098add1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ff451546dab4413a22926c1d0ef626b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "908ceb3775984baa9f462a6db7611c7e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "983a80897b844c85b4bc118d81c6617f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "98fd3d2619704051812999c9b00afae5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae05a5fda3364eeab7a9a5a233df299d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b60d5e513c9649ccab44969745151342": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b7f761f3cf5542ad89eac15a40dfab43": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce0640b089774bebbb485895abb5b6a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d7a76b188cf94565bc760b275c24c72b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dee1b73d01e449ef8088048b1dc5cbd3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0678b27edb64f62aaedbfee011be1ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4e207721fd914c30bd176393a3b43360",
       "IPY_MODEL_4d017abbab6a4607b8c98c1874b14fdb",
       "IPY_MODEL_7759081e382942ba91fa419b246e9f38"
      ],
      "layout": "IPY_MODEL_dee1b73d01e449ef8088048b1dc5cbd3"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
